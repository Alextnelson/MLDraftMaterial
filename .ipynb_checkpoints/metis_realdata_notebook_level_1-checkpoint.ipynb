{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  I. Its time to get real\n",
    "\n",
    "Lets try out our classification algorithms a few real datasets.  In this notebook you will re-use snippets of your code from the classification and cross-validation layer 1 notebooks to perform classification on real datasets.   \n",
    "\n",
    "------\n",
    "This isn't a notebook you just read - you'll need to complete several coding portions of it (either individually or in groups) and think about the questions posed herein in order to build up your intuitive understanding of these algorithms, as well as your practical ability to use them via scikit-learn.  Whenever you see 'TODO' please do perform the requested task.\n",
    "\n",
    "In other words, this is 'learning by discovery' notebook where you (either individually or in small groups) will start to build up your understanding of machine learning by doing real work and discussing it with your peers and instructors.  This is the best way to learn anything, far more effective than a book or lecture series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries we will need for this notebook's experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score   # an accuracy scoring function from scikit learn\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Examining our first real dataset\n",
    "\n",
    "Have you ever received an email from the Prince of Nigeria, who just needs you to wire him a few hundred bucks so he can make it to the U.S., and when he gets here he will give you tens of thousands of dollars in return?  Seems legit.\n",
    "\n",
    "Its bogus emails like this - otherwise known as 'spam' emails - that motivated one of the first applications of classification: spam detection.  Your email system probably has one baked right in.  Using ML this spam detector tries to root out crappy emails and automatically trash them for you to save you the trouble of sifting through garbage to get to your real messages.\n",
    "\n",
    "The first dataset we will apply our classification schemes too is a class spam email database consisting features extracted from around 5,000 real and spam emails [[0]](#bib_cell).  The input features for each email are simple counts of relevant words in thes subject of the email: e.g., the number of '!' or '$' signs used, the number of upper case letters used, etc.,  For a full description of the features [check out the description page located here](https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cracking open the data\n",
    "\n",
    "Lets load in the dataset and take a look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ORIGINAL DATASET\n",
    "spam_data = np.asarray(pd.read_csv(\"datasets/spambase_data.csv\", header = None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the labels on the data are located in the final column, so lets extract the input data and labels respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data_1 = spam_data[:,:-1]\n",
    "labels_1 = spam_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many points are in the datasets?  How many input features are there?  We can use numpy to see this as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the number of datapoints in the dataset\n",
    "print 'The number of datapoints = ' + str(np.shape(input_data_1)[0])\n",
    "print 'The number of input features in the dataset = ' + str(np.shape(input_data_1)[1] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Performing classification \n",
    "\n",
    "With our data transformed, we now apply the ML algorithms with cross-validation developed in the Layer 1 cross-validation notebook. \n",
    "\n",
    "In this section you will use code snippets from the classification and cross-validation notebooks and apply what you wrote there to performing classification on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Logistic regression\n",
    "To get things started, in the next cell we run the standard logistic regression linear classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classifier mislabeled 311 of 4601 points\n"
     ]
    }
   ],
   "source": [
    "# create the linear logistic regression classifier and plug it into the previous Python function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# fit our chosen classifier to the dataset\n",
    "classifier.fit(input_data_1, labels_1)                              \n",
    "\n",
    "# print out the number of misclassified points\n",
    "predicted_labels = classifier.predict(input_data_1)\n",
    "acc = len(labels_1) - accuracy_score(labels_1.ravel(), predicted_labels.ravel(), normalize=False)\n",
    "print 'Our classifier mislabeled ' + str(acc) + ' of ' + str(len(labels_1)) + ' points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad - lets try using a nonlinear algorithm cross-validated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated nonlinear classification algorithm\n",
    "\n",
    "Go back to your completed Layer 1 cross-validated notebook and copy the code for performing cross-validated classification - starting with any of the nonlinear methods you wish.  If you are working in a group it might be a good idea for each person to try a different algorithm, can then you can compare the results.\n",
    "\n",
    "Whichever algorithm you choose, use the same range of parameters tested in the layer 1 cross-validation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter according to cross-validation param is = 10\n"
     ]
    }
   ],
   "source": [
    "# TODO: import your nonlinear classification algorithm and make a default instance of it\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# create a parameter range to test over\n",
    "parameters = {'max_depth':np.arange(1,50)}\n",
    "\n",
    "# create an instance of the GridSearchCV cross-validator - using our classifier and choice or parameters\n",
    "cross_validator = GridSearchCV(classifier, parameters)\n",
    "\n",
    "# get the best parameter choice based on cross-validation and print out\n",
    "cross_validator.fit(input_data_1,labels_1)        \n",
    "best_param =  cross_validator.best_estimator_.max_depth    \n",
    "print 'Best parameter according to cross-validation param is = ' + str(best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best choice of parameter found via cross-validation, lets look at the resulting model.\n",
    "\n",
    "In the next cell we plot the result of a tree classifier fit to the data - plugging in the best_param found via cross-validation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our classifier mislabeled 163 of 4601 points\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(max_depth=best_param)\n",
    "\n",
    "# fit our chosen classifier to the dataset\n",
    "classifier.fit(input_data_1, labels_1)                              \n",
    "\n",
    "# print out the number of misclassified points\n",
    "predicted_labels = classifier.predict(input_data_1)\n",
    "acc = len(labels_1) - accuracy_score(labels_1.ravel(), predicted_labels.ravel(), normalize=False)\n",
    "print 'Our classifier mislabeled ' + str(acc) + ' of ' + str(len(labels_1)) + ' points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions -  TODO \n",
    "\n",
    "- What was the very best percentage (of points classified correctly) you were able to achieve?  Are you impressed with the results?  What do you think you could do to improve your results?\n",
    "\n",
    "I tried for a few varieties and actually scored the best with a decision tree that ended up only misclassifying 4 of 4601 points. Of course then I did a few more runs of the algorithm and now cannot seem to replicate that accuracy level so perhaps there was a glitch?\n",
    "\n",
    "- if you tried multiple algorithms, how did they compare in terms of speed of execution?\n",
    "\n",
    "Decision tree was the fastest in terms of processing while the others were fairly slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II.  Preprocessing your data can make a big difference\n",
    "\n",
    "Face detection (how your smartphone photo app finds your face before snapping a shot) is a two class classification problem: class 1 is a collection of faces, class 2 is a collection of small image patches.  This is drawn figuratively in the image below (used with permission from [[2]](#bib_cell).\n",
    "\n",
    "<img src=\"images/wright_bros_face_detect.png\" width=500 height=500/>\n",
    "\n",
    "In this exercise you are going to compare how accurately you can distinguish between a large example of facial and non-facial images when you a) simply use raw pixel values as input and b) when you use a more descriptive feature extracted from each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Examining the dataset\n",
    "\n",
    "What does a face detection dataset look like?  Like the figure below - it consists of a set of image patches - some of which contain faces (one class) and some of them are non-faces (the second class).  \n",
    "\n",
    "<img src=\"images/face_detection_data.png\" width=400 height=400/>\n",
    "\n",
    "\n",
    "A gray-scale image is just a matrix of numbers - as illustrated below (used with permission from [[2]](#bib_cell). - and so each of these patches is reshaped into a vector for use with any classification algorithm. \n",
    "\n",
    "<img src=\"images/image_as_pixels.png\" width=400 height=400/>\n",
    "\n",
    "However - as we learn through experimentation here - using the raw pixel values themselves as input leads to poor classification accuracy when performing object detection.  On the contrary - extracting characteristic **features** from image data can lead to excellent performance in practice.  In particular, the feature we will be comparing our raw pixel data too is called a Histogram of Oriented Gradients feature (see e.g., [this short slide-deck](https://drive.google.com/file/d/0B9LZEwqBZcp4NzRwcGZ6WDhXdEE/view?usp=sharing) for an overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TODO:  Download the datasets\n",
    "\n",
    "Download the raw image patch database  - *raw_face_data.csv* - via the link below \n",
    "\n",
    "\n",
    "https://drive.google.com/open?id=0B3u28XMyXeBaNlUyQ1laSlZQSUE\n",
    "\n",
    "Download the extracted feature version of this same dataset - called *feat_face_data.csv* - via the link below\n",
    "\n",
    "\n",
    "https://drive.google.com/open?id=0B3u28XMyXeBaUHdibkFOSTd6T3M\n",
    "\n",
    "\n",
    "OPTIONAL TODO: Once you have both datasets, try plotting an input example from each version of the data as an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Performing logistic regression on both datasets\n",
    "\n",
    "Re-use code from your completed classification notebook and run a logistic regression classifier on both datasets, computing the number of misclassified points in each case.  \n",
    "\n",
    "NOTE: do **not** perform any cross-validation, as these datasets are large enough that simply running one classifier on them can take several minutes on an average laptop.  On that note those with less powerful machines may need to *subsample* both datasets in order for the classifications to complete in reasonable time.m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: load in two instances of logistic regression from scikit-learn\n",
    "# and perform classification on both datasets, counting the number of \n",
    "# misclassified points in both instances.  How well can you classify \n",
    "# on both datasets?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  III. Formatting data for use with ML algos\n",
    "\n",
    "Lets try out our classification algorithms on another real dataset.  \n",
    "\n",
    "Most datasets - particularly those consisting of metadata like e.g., customer information, census data, etc., - do not come to us in a form we can immediately throw into an ML algorithm.  In this notebook we will look at two-class classification dataset of about 12,000 datapoints that needs to be 'cleaned' in various common ways so that it is properly formatted for us with ML algorithms.\n",
    "\n",
    "Once cleaned you will use the classification algorithms from the Layer 1 classification notebook on this dataset.  The results of these experimemts will lead us to a valuable set of conclusions about performing machine learning in practice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  Examining the dataset\n",
    "\n",
    "In the short story [Minority Report](https://en.wikipedia.org/wiki/The_Minority_Report) by Phillip K Dick, police of the future have a technology that allows them to accurately predict crimes before they occur ('future-crime').  They use this tech to hunt down individuals and arrest them before they commit a crime.  The ending of this story - like many of Dick's (e.g., Do Androids Dream of Electric Sheep - later adapted into the movie Bladerunner)  - isn't pretty, as the concept of predicting who and who will not become a future criminal is ripe with oppurtunties for corruption.\n",
    "\n",
    "But hey - that doesn't mean that absolutely every twist on this concept - i.e.,  predicting persons who will FILL IN THE BLANK - are all equally as concernting.  Here's one amiable twist - predicting persons who will need to be hospitalized in the near future, so that that these individuals can be contacted and preventative medicine can be used to help thm avoid potential ruin.\n",
    "\n",
    "The dataset [[1]](#bib_cell) we will look at in this notebook aims to create just this predictive ability.  In particular it contains around 12,000 patient records containing a number of features about each patient, as well as whether or not a patient returned to the hospital within 30 days.\n",
    "Our goal will be to use ML algorithms to try to predict which patients will in fact return within 30 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cracking open the data\n",
    "\n",
    "Lets load in the dataset and take a look at what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ORIGINAL DATASET\n",
    "medical_data = pd.read_csv(\"datasets/balanced_cleaned_diabetes_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print dataset for visual analysis\n",
    "medical_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.**  The columns here are the input - or 'features' - of the dataset are \n",
    "\n",
    "gender - gender of patient\n",
    "\n",
    "age - age of patient, split into 10 year chunks\n",
    "\n",
    "number_emergency - Number of emergency visits of the patient in the year preceding the encounter\n",
    "\n",
    "**2.**  The target / labels are\n",
    "\n",
    "readmitted - target variable, consists of '<30' (readmitted \n",
    "within 30 days), '>30' (readmitted after 30 days), and NO (no history of readmission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need to do to clean this dataset?\n",
    "\n",
    "Lets think about what we have here.\n",
    "\n",
    "**1. With regard to the input features**\n",
    "\n",
    "- age - in its raw form ('(10-20)', '(20-30)',etc.,) cannot be proccessed by any ML algorithm.  We will need to transform these appropiately - and one way is to represent each range with an integer range as \n",
    "\n",
    "'(10-20)' --> 0\n",
    "\n",
    "'(20-30)' --> 1\n",
    "\n",
    "'(30-40)' --> 2\n",
    "\n",
    "etc..,\n",
    "\n",
    "'(90-100)' --> 9\n",
    "\n",
    "\n",
    "    Note: because this variable is ordered we want the transformation - in this case integers - to retain this ordering.\n",
    "\n",
    "\n",
    "- gender - in its raw form ('Male', 'Female') cannot be processed by any ML algorithm.  We will need to transform it via a process known as ['dummying'](https://github.com/jermwatt/mlrefined/blob/master/4.1_Handling_categorical_features%20.ipynb).  This converts this **categorical feature** into a binary form  ML algorithms can process.\n",
    "\n",
    "\n",
    "**2.  With regard to the input features**\n",
    "\n",
    "Since we are interested in predicting whether or not a patient will be readmitted in the next 30 days, patients with either the label '>30' and 'NO' count as a single class (those who do not return within 30 days).  So we will need to merge these two sets of patients - so that we have only two distinct label types.\n",
    "\n",
    "Additionally in its raw form the labels are not useful for an ML algorithm - we will need to transform them into integer values.  We will do this as follows\n",
    "\n",
    "'>30' or 'NO' --> -1\n",
    "\n",
    "'<30' --> +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets get to work\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "Follow the instructions above to convert the input features and target / labels so that they can be processed by an ML algorithm.\n",
    "\n",
    "Hint: some of the following functionality from the pandas and numpy libraries may be helpful in doing this.  Note: this is not at all an exhaustive list, there are many ways of transforming a dataset like this one.\n",
    "\n",
    "- **from pandas:** replace, get_dummies, copy\n",
    "\n",
    "- **from numpy:** unique, argwhere\n",
    "\n",
    "\n",
    "Once complete your transformed dataset should look something like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in a transformed version of the dataset - yours should look something like this\n",
    "transformed_medical_data = pd.read_csv('datasets/transformed_diabetes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed_medical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: transform the input features and target / labels so we can use ML algorithms on the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Performing classification \n",
    "\n",
    "With our data transformed, we now apply the ML algorithms with cross-validation developed in the Layer 1 cross-validation notebook. \n",
    "\n",
    "In this section you will use code snippets from the classification and cross-validation notebooks and apply what you wrote there to performing classification on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic regression\n",
    "To get things started, in the next cell we run the standard logistic regression linear classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pull out the input feature and target / labels \n",
    "input_data_1 = transformed_medical_data[['Male','Female','age','number_emergency']]\n",
    "labels_1 = transformed_medical_data['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the linear logistic regression classifier and plug it into the previous Python function\n",
    "from sklearn.metrics import accuracy_score   # an accuracy scoring function from scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# fit our chosen classifier to the dataset\n",
    "classifier.fit(input_data_1, labels_1)                              \n",
    "\n",
    "# print out the number of misclassified points\n",
    "predicted_labels = classifier.predict(input_data_1)\n",
    "acc = len(labels_1) - accuracy_score(labels_1.ravel(), predicted_labels.ravel(), normalize=False)\n",
    "print 'Our classifier mislabeled ' + str(acc) + ' of ' + str(len(labels_1)) + ' points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we only correctly classify \n",
    "\n",
    "$\\frac{15628 - 6725}{15628} = 57\\%$  \n",
    "\n",
    "of the data.  Not too good!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated tree-based algorithm\n",
    "\n",
    "Go back to your completed Layer 1 cross-validated notebook and copy the code for performing cross-validated tree-based classification.  Lets see if we can't get a better result.  Here we use the same range of the parameter 'max_depth' parameter to search over, as we did in the Layer 1 cross-validation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: import your tree-based algorithm and make a default instance of it\n",
    "\n",
    "\n",
    "# range of parameters to test with cross-validation\n",
    "parameters = {'max_depth': np.arange(1,10)}\n",
    "\n",
    "# create an instance of the GridSearchCV cross-validator - using our classifier and choice or parameters\n",
    "cross_validator = GridSearchCV(classifier, parameters)\n",
    "\n",
    "# get the best parameter choice based on cross-validation and print out\n",
    "cross_validator.fit(input_data_1,labels_1)        \n",
    "best_param =  cross_validator.best_estimator_.max_depth     \n",
    "print 'best parameter according to cross-validation param is = ' + str(best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the best choice of parameter found via cross-validation, lets look at the resulting model.\n",
    "\n",
    "In the next cell we plot the result of a tree classifier fit to the data - plugging in the best_param found via cross-validation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: create an instance of a tree-based classifier from scikit learn\n",
    "\n",
    "\n",
    "# fit our chosen classifier to the dataset\n",
    "classifier.fit(input_data_1, labels_1)                              \n",
    "\n",
    "# print out the number of misclassified points\n",
    "predicted_labels = classifier.predict(input_data_1)\n",
    "acc = len(labels_1) - accuracy_score(labels_1.ravel(), predicted_labels.ravel(), normalize=False)\n",
    "print 'Our classifier mislabeled ' + str(acc) + ' of ' + str(len(labels_1)) + ' points'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "\n",
    "- What percentage of the data did you correctly classify with your cross-validated tree based algorithm?\n",
    "\n",
    "\n",
    "- If you would like to try kernels / neural network based algorithms create new Python cells below and follow the general outline used above.  Beware: these will likely take considerably longer to process than your tree-based algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions -  TODO \n",
    "\n",
    "- What was the very best percentage (of points classified correctly) you were able to achieve?  Are you impressed with the results?  What do you think you could do to improve your results?\n",
    "\n",
    "- if you tried multiple algorithms, how did they compare in terms of speed of execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "A little underwhelmed with your results?  \n",
    "\n",
    "Our results on this dataset present an important practical principle for using ML in practice: **ML doesn't always work the way you want it too.**  Why is that?  **Because sometimes there's no pattern your data to find.**  What do you do if your dataset has no pattern in it?  **You have to find or create another one.**  What if you don't have access to another dataset?  Simple: you move on to the next problem.\n",
    "\n",
    "The short hand for this is **Garbage In Garbage Out** (GIGO).  There is no magic bullet for GIGO.  \n",
    "\n",
    ">**One of the Great Cardinal Sins you will see when you go out into the ML world are people / organizations grinding away - trying all sorts of esoteric algorithms and tricks - to try to get a high accuracy on a dataset of garbage.**\n",
    "\n",
    "Take for example the toy dataset generated by the following Python cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate a toy classification dataset completely jumbled up\n",
    "X = np.random.rand(300,2)\n",
    "labels = np.sign(np.random.randn(300))\n",
    "ind = np.argwhere(labels == 1)\n",
    "ind = [s[0] for s in ind]\n",
    "ind2 = np.argwhere(labels == -1)\n",
    "ind2 = [s[0] for s in ind2]\n",
    "\n",
    "# plot toy dataset\n",
    "plt.scatter(X[ind,0],X[ind,1],color = 'cornflowerblue',s = 40,linewidth = 1,edgecolor = 'k')\n",
    "plt.scatter(X[ind2,0],X[ind2,1],color = 'salmon',s = 40, linewidth = 1,edgecolor = 'k')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "\n",
    "# clean up plot\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This two-class toy dataset is completely random = there's no real pattern here.  No machine learning algorithm can perform well on this sort of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bib_cell'></a>\n",
    "\n",
    "# Bibliiography and notes\n",
    "\n",
    "[0] This dataset is a slightly cleaned up version of the one located here https://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "[1] The dataset is a cleaned and balanced subset of the original dataset available here\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Diabetes+130-US+hospitals+for+years+1999-2008\n",
    "\n",
    "Which was first introduced in the following paper.\n",
    "\n",
    "Strack, Beata, et al. \"[Impact of HbA1c measurement on hospital readmission rates: analysis of 70,000 clinical database patient records.](http://www.hindawi.com/journals/bmri/2014/781670/)\" BioMed research international 2014 (2014).\n",
    "\n",
    "[2] Watt, Jeremy et al. [Machine Learning Refined](www.mlrefined.com). Cambridge University Press, 2016"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
